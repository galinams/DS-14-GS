{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SCIENCE AT-SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO WORK\n",
    "When you get to class ...\n",
    "- Open DS-DC-14_11_data_science_at_scale.ipynb\n",
    "- run `conda install dask` to get a copy of the Dask library for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA SCIENCE AT SCALE LEARNING OBJECTIVES\n",
    "\n",
    "- Define Big Data and its challenges\n",
    "- Learn how to deal with data that doesn't fit into memory\n",
    "- Understand why Hadoop and Spark exist\n",
    "- Get an introduction to using Spark and MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUIDED PRACTICE\n",
    "\n",
    "Let's continue the exercise from last class so we have a chance to go through the full data science workflow as a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTIVITY: PROJECT PRACTICE\n",
    "Objective: Review content thus far in the course, bring the material together, find weak areas, and get help in class.\n",
    "\n",
    "Using the flights data from the last example try to make it through the data science workflow in the first hour of class.\n",
    "\n",
    "There are many ways to manipulate this data set: \n",
    "- Consider what is a proper \"categorical\" variable, and keep only what is significant. \n",
    "- You will easily have 20+ variables (start with one). Aim to have at least three visuals that clearly explain the relationship of variables you've used against the predictive survival value.\n",
    "- Generate the AUC or precision-recall curve (based on which you think makes more sense), and have a statement that defines, compared to a baseline, how your model performs and any caveats.  \n",
    "  - For example: \"My model on average performs at x rate, but the features under-perform and explain less of the data at these thresholds.\" Consider this as practice for your own project, since the steps you'll take to present your work will be relatively similar.\n",
    "  \n",
    "[Pandas super cheatsheet](http://nbviewer.jupyter.org/github/justmarkham/pandas-videos/blob/master/pandas.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.linear_model as lm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('assets/dataset/flight_delays.csv')\n",
    "\n",
    "df = df.join(pd.get_dummies(df['DAY_OF_WEEK'], prefix='dow'))\n",
    "df = df[df.DEP_DEL15.notnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.UNIQUE_CARRIER.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.join(pd.get_dummies(df['UNIQUE_CARRIER'], prefix='airline'))\n",
    "df = df[df.DEP_DEL15.notnull()].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('UNIQUE_CARRIER', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['airline_AA','airline_AS','airline_DL','airline_EV','airline_F9', 'airline_HA','airline_MQ','airline_NK', 'airline_OO','airline_UA', 'airline_US', 'airline_VX', 'airline_WN' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X= df[features]\n",
    "y= df['DEP_DEL15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = cross_validation.train_test_split(X,y, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = linear_model.LogisticRegression().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802632267975\n"
     ]
    }
   ],
   "source": [
    "print lm.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      1.00      0.89     91964\n",
      "        1.0       0.00      0.00      0.00     22614\n",
      "\n",
      "avg / total       0.64      0.80      0.71    114578\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\korol_000\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print metrics.classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802632267975\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print metrics.accuracy_score(y_test, y_pred)\n",
    "print metrics.roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[91964     0]\n",
      " [22614     0]]\n"
     ]
    }
   ],
   "source": [
    "print metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dm = dummy.DummyClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred1= dm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print metrics.accuracy_score(y_test, y_pred1)\n",
    "print metrics.roc_auc_score(y_test, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802632267975\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=10, max_depth=8)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print metrics.accuracy_score(y_test, y_pred)\n",
    "print metrics.roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAT IS OUT OF CORE LEARNING?\n",
    "Objective: Learn how to deal with the problem of too much data on one computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to train models that don't fit into main memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For our purposes we can separate computer memory into two categories**\n",
    "- Random Access Memory (RAM) -- Main/Core memory\n",
    "- Hard Disk (HDD/SDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAM**\n",
    "- About 1000x faster than hard disk\n",
    "- Expensive and comes with space limitations \n",
    "  - It's difficult to get more than 200GB of RAM on a single computer/server\n",
    "  - Much less on most computers (16GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HDD/SDD**\n",
    "- About 1000x slower than RAM, although newer SDD drives are much better\n",
    "- Could possible store up to 5TB on a single computer/server\n",
    "  - Much less on most computers (1TB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem: By default we want to work within RAM because this is fast. This poses some issues for larger datasets because we can't fit them into RAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions**\n",
    "- Get more RAM\n",
    "  - One hour of a developer's time can buy 1 month of 8GB of computing power.\n",
    "- Sample your data\n",
    "  - For most of the data science workflow you can work on a small sample of your data without affecting results\n",
    "- Use clever algorithms to store data on harddisk and do processing in RAM\n",
    "  - Unix command-line tools\n",
    "  - Blaze/Dask\n",
    "  - Many other options, however, they tend to be less user-friendly than in RAM tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNOWLEDGE CHECK\n",
    "\n",
    "I just recieved a 100GB csv file, what should I do with it if I want to explore it?\n",
    "\n",
    "<!--\n",
    "ANSWER:\n",
    "Don't try to read in the whole file. You only need to preview the columns and sample some rows for preview.\n",
    "\n",
    "You can do this with command line using head, tail, and awk\n",
    "- To get you started: http://bconnelly.net/working-with-csvs-on-the-command-line/\n",
    "\n",
    "You can also do this in Python:\n",
    "- import csv library to read line by line\n",
    "- Use the chunksize argument for pd.read_csv to read chunk by chunk\n",
    "- Use Dask, which we will cover today\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO: USE DASK TO FIT A LOGISTIC REGRESSION OUT OF CORE USING STOCHASTIC GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "# Read 1 MB at a time to keep main memory use low\n",
    "# The flight delays data is 43 MB on disk so we should have 43 divisions\n",
    "df = dd.read_csv('assets/dataset/flight_delays.csv', blocksize=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>UNIQUE_CARRIER</th>\n",
       "      <th>AIRLINE_ID</th>\n",
       "      <th>CARRIER</th>\n",
       "      <th>ORIGIN_AIRPORT_ID</th>\n",
       "      <th>ORIGIN_AIRPORT_SEQ_ID</th>\n",
       "      <th>ORIGIN_CITY_MARKET_ID</th>\n",
       "      <th>DEST_AIRPORT_ID</th>\n",
       "      <th>DEST_AIRPORT_SEQ_ID</th>\n",
       "      <th>DEST_CITY_MARKET_ID</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DEL15</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>AA</td>\n",
       "      <td>19805</td>\n",
       "      <td>AA</td>\n",
       "      <td>12478</td>\n",
       "      <td>1247802</td>\n",
       "      <td>31703</td>\n",
       "      <td>12892</td>\n",
       "      <td>1289203</td>\n",
       "      <td>32575</td>\n",
       "      <td>900.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>AA</td>\n",
       "      <td>19805</td>\n",
       "      <td>AA</td>\n",
       "      <td>12892</td>\n",
       "      <td>1289203</td>\n",
       "      <td>32575</td>\n",
       "      <td>12478</td>\n",
       "      <td>1247802</td>\n",
       "      <td>31703</td>\n",
       "      <td>900.0</td>\n",
       "      <td>856.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>AA</td>\n",
       "      <td>19805</td>\n",
       "      <td>AA</td>\n",
       "      <td>12478</td>\n",
       "      <td>1247802</td>\n",
       "      <td>31703</td>\n",
       "      <td>12892</td>\n",
       "      <td>1289203</td>\n",
       "      <td>32575</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>1226.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>AA</td>\n",
       "      <td>19805</td>\n",
       "      <td>AA</td>\n",
       "      <td>12892</td>\n",
       "      <td>1289203</td>\n",
       "      <td>32575</td>\n",
       "      <td>12478</td>\n",
       "      <td>1247802</td>\n",
       "      <td>31703</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>1214.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>AA</td>\n",
       "      <td>19805</td>\n",
       "      <td>AA</td>\n",
       "      <td>11298</td>\n",
       "      <td>1129803</td>\n",
       "      <td>30194</td>\n",
       "      <td>12173</td>\n",
       "      <td>1217302</td>\n",
       "      <td>32134</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>1754.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DAY_OF_MONTH  DAY_OF_WEEK     FL_DATE UNIQUE_CARRIER  AIRLINE_ID CARRIER  \\\n",
       "0             1            4  2015-01-01             AA       19805      AA   \n",
       "1             1            4  2015-01-01             AA       19805      AA   \n",
       "2             1            4  2015-01-01             AA       19805      AA   \n",
       "3             1            4  2015-01-01             AA       19805      AA   \n",
       "4             1            4  2015-01-01             AA       19805      AA   \n",
       "\n",
       "   ORIGIN_AIRPORT_ID  ORIGIN_AIRPORT_SEQ_ID  ORIGIN_CITY_MARKET_ID  \\\n",
       "0              12478                1247802                  31703   \n",
       "1              12892                1289203                  32575   \n",
       "2              12478                1247802                  31703   \n",
       "3              12892                1289203                  32575   \n",
       "4              11298                1129803                  30194   \n",
       "\n",
       "   DEST_AIRPORT_ID  DEST_AIRPORT_SEQ_ID  DEST_CITY_MARKET_ID  CRS_DEP_TIME  \\\n",
       "0            12892              1289203                32575         900.0   \n",
       "1            12478              1247802                31703         900.0   \n",
       "2            12892              1289203                32575        1230.0   \n",
       "3            12478              1247802                31703        1220.0   \n",
       "4            12173              1217302                32134        1305.0   \n",
       "\n",
       "   DEP_TIME  DEP_DEL15  Unnamed: 15  \n",
       "0     855.0        0.0          NaN  \n",
       "1     856.0        0.0          NaN  \n",
       "2    1226.0        0.0          NaN  \n",
       "3    1214.0        0.0          NaN  \n",
       "4    1754.0        1.0          NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.Series<describ..., npartitions=1>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oops, this won't give you results, because you need to bring the results into RAM explicitly\n",
    "df.DAY_OF_MONTH.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\korol_000\\Anaconda2\\lib\\site-packages\\numpy\\lib\\function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    458311.000000\n",
       "mean       1333.028542\n",
       "std         479.639617\n",
       "min           1.000000\n",
       "25%                NaN\n",
       "50%                NaN\n",
       "75%                NaN\n",
       "max        2400.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.DEP_TIME.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4598.000000\n",
       "mean     1333.053936\n",
       "std       476.600229\n",
       "min         3.000000\n",
       "25%      1214.000000\n",
       "50%       905.250000\n",
       "75%              NaN\n",
       "max      2358.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That took a while, it's smart to use sampling where possible\n",
    "# Especially for Exploratory data analysis\n",
    "# 30 samples can give a solid estimate \n",
    "# Reducing the work load to 4,000 isn't too statistically different than 400,000\n",
    "df.DEP_TIME.sample(.01).describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You may have noticed that took just as long because Dask needed time to sample the data\n",
    "# You only need to sample once\n",
    "sample_df = df.sample(.01).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>AIRLINE_ID</th>\n",
       "      <th>ORIGIN_AIRPORT_ID</th>\n",
       "      <th>ORIGIN_AIRPORT_SEQ_ID</th>\n",
       "      <th>ORIGIN_CITY_MARKET_ID</th>\n",
       "      <th>DEST_AIRPORT_ID</th>\n",
       "      <th>DEST_AIRPORT_SEQ_ID</th>\n",
       "      <th>DEST_CITY_MARKET_ID</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DEL15</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4710.000000</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>4.710000e+03</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>4.710000e+03</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>4577.000000</td>\n",
       "      <td>4577.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.850318</td>\n",
       "      <td>4.022505</td>\n",
       "      <td>19975.247771</td>\n",
       "      <td>12664.241401</td>\n",
       "      <td>1.266427e+06</td>\n",
       "      <td>31722.077707</td>\n",
       "      <td>12685.657325</td>\n",
       "      <td>1.268568e+06</td>\n",
       "      <td>31722.953079</td>\n",
       "      <td>1328.595541</td>\n",
       "      <td>1338.201224</td>\n",
       "      <td>0.199039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.944432</td>\n",
       "      <td>1.934784</td>\n",
       "      <td>403.007598</td>\n",
       "      <td>1532.472012</td>\n",
       "      <td>1.532469e+05</td>\n",
       "      <td>1287.284303</td>\n",
       "      <td>1506.319106</td>\n",
       "      <td>1.506316e+05</td>\n",
       "      <td>1286.056786</td>\n",
       "      <td>468.347694</td>\n",
       "      <td>478.183937</td>\n",
       "      <td>0.399321</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19393.000000</td>\n",
       "      <td>10135.000000</td>\n",
       "      <td>1.013503e+06</td>\n",
       "      <td>30113.000000</td>\n",
       "      <td>10135.000000</td>\n",
       "      <td>1.013503e+06</td>\n",
       "      <td>30073.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>19790.000000</td>\n",
       "      <td>11292.000000</td>\n",
       "      <td>1.129202e+06</td>\n",
       "      <td>30627.000000</td>\n",
       "      <td>11292.000000</td>\n",
       "      <td>1.129202e+06</td>\n",
       "      <td>30647.000000</td>\n",
       "      <td>930.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>19977.000000</td>\n",
       "      <td>12889.000000</td>\n",
       "      <td>1.288903e+06</td>\n",
       "      <td>31453.000000</td>\n",
       "      <td>12889.000000</td>\n",
       "      <td>1.288903e+06</td>\n",
       "      <td>31453.000000</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>20366.000000</td>\n",
       "      <td>13930.000000</td>\n",
       "      <td>1.393003e+06</td>\n",
       "      <td>32467.000000</td>\n",
       "      <td>14027.000000</td>\n",
       "      <td>1.402702e+06</td>\n",
       "      <td>32467.000000</td>\n",
       "      <td>1720.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>21171.000000</td>\n",
       "      <td>16218.000000</td>\n",
       "      <td>1.621801e+06</td>\n",
       "      <td>35991.000000</td>\n",
       "      <td>15991.000000</td>\n",
       "      <td>1.599102e+06</td>\n",
       "      <td>35991.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DAY_OF_MONTH  DAY_OF_WEEK    AIRLINE_ID  ORIGIN_AIRPORT_ID  \\\n",
       "count   4710.000000  4710.000000   4710.000000        4710.000000   \n",
       "mean      15.850318     4.022505  19975.247771       12664.241401   \n",
       "std        8.944432     1.934784    403.007598        1532.472012   \n",
       "min        1.000000     1.000000  19393.000000       10135.000000   \n",
       "25%        8.000000     2.000000  19790.000000       11292.000000   \n",
       "50%       16.000000     4.000000  19977.000000       12889.000000   \n",
       "75%       23.000000     6.000000  20366.000000       13930.000000   \n",
       "max       31.000000     7.000000  21171.000000       16218.000000   \n",
       "\n",
       "       ORIGIN_AIRPORT_SEQ_ID  ORIGIN_CITY_MARKET_ID  DEST_AIRPORT_ID  \\\n",
       "count           4.710000e+03            4710.000000      4710.000000   \n",
       "mean            1.266427e+06           31722.077707     12685.657325   \n",
       "std             1.532469e+05            1287.284303      1506.319106   \n",
       "min             1.013503e+06           30113.000000     10135.000000   \n",
       "25%             1.129202e+06           30627.000000     11292.000000   \n",
       "50%             1.288903e+06           31453.000000     12889.000000   \n",
       "75%             1.393003e+06           32467.000000     14027.000000   \n",
       "max             1.621801e+06           35991.000000     15991.000000   \n",
       "\n",
       "       DEST_AIRPORT_SEQ_ID  DEST_CITY_MARKET_ID  CRS_DEP_TIME     DEP_TIME  \\\n",
       "count         4.710000e+03          4710.000000   4710.000000  4577.000000   \n",
       "mean          1.268568e+06         31722.953079   1328.595541  1338.201224   \n",
       "std           1.506316e+05          1286.056786    468.347694   478.183937   \n",
       "min           1.013503e+06         30073.000000      5.000000     2.000000   \n",
       "25%           1.129202e+06         30647.000000    930.000000          NaN   \n",
       "50%           1.288903e+06         31453.000000   1323.000000          NaN   \n",
       "75%           1.402702e+06         32467.000000   1720.000000          NaN   \n",
       "max           1.599102e+06         35991.000000   2359.000000  2400.000000   \n",
       "\n",
       "         DEP_DEL15  Unnamed: 15  \n",
       "count  4577.000000          0.0  \n",
       "mean      0.199039          NaN  \n",
       "std       0.399321          NaN  \n",
       "min       0.000000          NaN  \n",
       "25%            NaN          NaN  \n",
       "50%            NaN          NaN  \n",
       "75%            NaN          NaN  \n",
       "max       1.000000          NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Much faster\n",
    "sample_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model fit changes as well, you need to feed in the data chunk by chunk\n",
    "# This means that some models cannot work out of core\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent with a log-loss is logistic regression\n",
    "sgd = linear_model.SGDClassifier(loss='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[['DEP_TIME', 'DEP_DEL15']]\n",
    "df = df.dropna(subset=['DEP_DEL15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the number of divisions\n",
    "divs = len(df.divisions) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division:  0 [-723.89493824] [[ 363.43804538]]\n",
      "Division:  1 [-984.20131488] [[-498.94129837]]\n",
      "Division:  2 [-1198.58423557] [[-319.33309038]]\n",
      "Division:  3 [-1375.23714976] [[ 65.32571009]]\n",
      "Division:  4 [-1522.42858587] [[-116.25029569]]\n",
      "Division:  5 [-1642.42131475] [[-27.43357549]]\n",
      "Division:  6 [-1702.37768073] [[ 239.50904152]]\n",
      "Division:  7 [-1775.80744095] [[ 221.4808159]]\n",
      "Division:  8 [-1835.67327596] [[-67.47479507]]\n",
      "Division:  9 [-1875.84238331] [[ 40.86592996]]\n",
      "Division:  10 [-1911.73675281] [[ 7.16831207]]\n",
      "Division:  11 [-1944.47413797] [[-1.67357044]]\n",
      "Division:  12 [-1972.76390958] [[-23.42563066]]\n",
      "Division:  13 [-1987.83137173] [[-44.64415282]]\n",
      "Division:  14 [-2011.22619876] [[-47.88477093]]\n",
      "Division:  15 [-2042.09682559] [[-20.94050061]]\n",
      "Division:  16 [-2070.74434542] [[ 116.91780076]]\n",
      "Division:  17 [-2079.76657809] [[ 26.16944056]]\n",
      "Division:  18 [-2087.43747343] [[-10.60659786]]\n",
      "Division:  19 [-2095.57291176] [[-27.59208108]]\n",
      "Division:  20 [-2103.4961846] [[-65.80390923]]\n",
      "Division:  21 [-2116.44753251] [[ 9.22620742]]\n",
      "Division:  22 [-2127.34152464] [[-7.65818524]]\n",
      "Division:  23 [-2131.38432127] [[ 8.25269918]]\n",
      "Division:  24 [-2138.84819119] [[-44.10538522]]\n",
      "Division:  25 [-2145.5865524] [[-61.20533333]]\n",
      "Division:  26 [-2152.21083059] [[-22.07776507]]\n",
      "Division:  27 [-2156.25972053] [[-11.52762674]]\n",
      "Division:  28 [-2163.13122043] [[-24.52040922]]\n",
      "Division:  29 [-2169.55332782] [[-26.54171003]]\n",
      "Division:  30 [-2177.20720918] [[ 30.27075348]]\n",
      "Division:  31 [-2183.46116474] [[-16.0250608]]\n",
      "Division:  32 [-2189.86698466] [[-31.56450353]]\n",
      "Division:  33 [-2193.11369181] [[-3.79794122]]\n",
      "Division:  34 [-2199.2819465] [[-16.42884244]]\n",
      "Division:  35 [-2205.11961947] [[-20.60044669]]\n",
      "Division:  36 [-2208.4505232] [[-37.06016014]]\n",
      "Division:  37 [-2209.33252293] [[ 2.00509286]]\n",
      "Division:  38 [-2211.57432812] [[ 0.5256402]]\n",
      "Division:  39 [-2215.59865522] [[-40.03777492]]\n",
      "Division:  40 [-2220.27085384] [[-35.52899115]]\n",
      "Division:  41 [-2227.41019395] [[-0.70731177]]\n",
      "Division:  42 [-2233.12153085] [[-33.43755758]]\n",
      "Division:  43 [-2234.36910846] [[-11.03905431]]\n"
     ]
    }
   ],
   "source": [
    "# Iterate through divisions and update model fit\n",
    "for n in range(divs):\n",
    "    # Get a chunk of rows\n",
    "    ndf = df.get_partition(n).compute()\n",
    "    \n",
    "    # Pull out features and outcome\n",
    "    X = ndf[['DEP_TIME']]\n",
    "    y = ndf['DEP_DEL15']\n",
    "    sgd.partial_fit(X, y, classes=[0, 1])\n",
    "    print 'Division: ', n, sgd.intercept_, sgd.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNOWLEDGE CHECK\n",
    "\n",
    "1. Why is the coefficient changing as we add more divisions? \n",
    "2. What happens to the confidence interval of our estimate?\n",
    "3. Can we overfit by training on too much data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Out-of-core learning in Python can be a bit difficult**\n",
    "\n",
    "Unfortunately, dask is not fully integrated with scikit-learn yet. So, you would need to make custom implementations of train_test_split or GridSearchCV\n",
    "- To get you started:\n",
    "    - https://github.com/dask/dask-learn\n",
    "    - http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html\n",
    "        \n",
    "**Some alternatives that take the idea of out of core learning further**\n",
    "- [MLlib](http://spark.apache.org/mllib/)\n",
    "- [Mahout](http://mahout.apache.org/)\n",
    "- [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki)\n",
    "\n",
    "**Sanity Check**\n",
    "- Most data science and machine learning problems start out as larger than main memory and are reduced to in memory by the time you begin to build models. So, the lack of dask and scikit-learn integration is not a likely limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAT IS BIG DATA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Learn how to deal with the problem of too much data on several computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've talked about data that can't fit into RAM, what about data that can't fit within a 5TB hard-disk or needs to be computed quickly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT IS BIG DATA AND ITS CHALLENGES?**\n",
    "- Big Data is data that is too big to process or store on a single machine\n",
    "\n",
    "**Processing Challenge:**\n",
    "- Data is growing faster than CPU speeds\n",
    "\n",
    "**Storage Challenge:**\n",
    "- Data is growing faster than single-machine storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution (at the moment)\n",
    "\n",
    "Distributed Computing / Cloud Computing / Cluster Computing\n",
    "\n",
    "- Use a large number of commodity HW\n",
    "- Not as expensive as premium hardware (super computers)\n",
    "- Easy to add capacity\n",
    "- Easy to mix and match HW\n",
    "- Cheaper per CPU/disk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distributed Computing Challenges**\n",
    "\n",
    "**HardWare challenges:**\n",
    "- Slow machines\n",
    "- Uneven capacity and performance\n",
    "- HW failures\n",
    "- Hard drives, memory, etc all fails\n",
    "- Increased latency\n",
    "- Network speeds slower than bus speeds\n",
    "\n",
    "**SoftWare challenge:**\n",
    "- How do you program and distribute work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNOWLEDGE CHECK\n",
    "Is RAM vs Hard Disk still an issue when we distribute our work to multiple computers?\n",
    "\n",
    "<!--\n",
    "Answer:\n",
    "Definitely. In fact, the trade-off between the two storage types is what inspires Spark\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HADOOP MAP REDUCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Map Reduce** is a two-phase divide and conquer algorithm invented and published by Google in 2004.\n",
    "\n",
    "1. In the mapper phase, data is split into chunks and the same operation is performed on each chunk, while\n",
    "2. In the reducer phase, data is aggregated back to produce a final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Map-Reduce](assets/images/map-reduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a really obvious idea from a programming perspective, but making it convenient to use is quite an accomplishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5]\n",
      "[2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "numbers = [1, 2, 3, 4]\n",
    "def add_one(x):\n",
    "    return x + 1\n",
    "\n",
    "print map(lambda x: x + 1, numbers)\n",
    "print map(add_one, numbers)\n",
    "# What's a lambda?\n",
    "# Just a syntax for making a short function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "numbers = [1, 2, 3, 4]\n",
    "def iter_sum(a, b):\n",
    "    return a + b\n",
    "\n",
    "print reduce(lambda a, b: a + b, numbers)\n",
    "print reduce(iter_sum, numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "numbers = [1, 2, 3, 4]\n",
    "print reduce(\n",
    "    lambda a, b: a + b,\n",
    "    map(lambda x: x + 1, \n",
    "    numbers)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does this help?**\n",
    "- Every mapper can be run on a separate computer\n",
    "  - Awesome, we can distribute work over several computers\n",
    "- However, we still need the final result to fit on a single computer after the reducer step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word-count](assets/images/word-count.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APACHE SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apache Spark** is a scalable, efficient improvement to Map Reduce that was developed at Berkeley AMPLab in 2010\n",
    "\n",
    "Uses in-memory computing (RAM) to increase performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Apache vs Hadoop](assets/images/apache_vs_hadoop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAP REDUCE VS SPARK**\n",
    "\n",
    "- Disk IO is much slower than Memory\n",
    "- Keep more data in-memory\n",
    "- Disk IO is in the order of milliseconds\n",
    "- Memory IO is in the order of nanoseconds\n",
    "\n",
    "**Other Improvements:**\n",
    "- Support for additional operations in addition to MR\n",
    "- Support for interactive and streaming processing in addition to batch\n",
    "- Support for Scala, R, and Python in addition to Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE\n",
    "DEMO AND INDEPENDENT PRACTICE ON DATABRICKS CLOUD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ga-students/DS-DC-14/tree/master/lessons/lesson-11/starter-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC REVIEW: DATA SCIENCE AT SCALE\n",
    "\n",
    "\n",
    "- What is Big Data and what are its challenges?\n",
    "- What are the differences between Map Reduce and Spark?\n",
    "- What are the differences between Spark RDDs and Spark DataFrames?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPCOMING WORK\n",
    "- Final Project Milestone 2 will be due Monday, September 12th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Syllabus](assets/images/syllabus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![timeline](assets/images/timeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESSON: DATA SCIENCE AT SCALE\n",
    "\n",
    "EXIT TICKET\n",
    "\n",
    "DON’T FORGET TO FILL OUT YOUR EXIT TICKET\n",
    "\n",
    "http://goo.gl/forms/gG5qAw9QljgkHC2q1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
